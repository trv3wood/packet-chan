"""Streamlit app: upload files, ingest into Chroma, ask queries."""
import streamlit as st
from dotenv import load_dotenv
import os
import uuid
import time

from ingest import save_uploaded_file, extract_text_from_file, chunk_text
from vector_store import VectorStore
from retriever import build_prompt
from llm_adapter import generate, get_llm_status

load_dotenv()

CHROMA_DIR = os.getenv("CHROMA_PERSIST_DIR", "./chroma_db")


@st.cache_resource
def get_store():
    return VectorStore(persist_directory=CHROMA_DIR)


def ingest_and_index(uploaded_files):
    """å¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        uploaded_files: å•ä¸ªæ–‡ä»¶æˆ–æ–‡ä»¶åˆ—è¡¨
    """
    # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
    if not isinstance(uploaded_files, list):
        uploaded_files = [uploaded_files]
    
    # é™åˆ¶æœ€å¤š5ä¸ªæ–‡ä»¶
    if len(uploaded_files) > 5:
        st.error(f"æœ€å¤šåªèƒ½å¤„ç†5ä¸ªæ–‡ä»¶ï¼Œæ‚¨ä¸Šä¼ äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        uploaded_files = uploaded_files[:5]
    
    # æ¸…é™¤ä¸Šä¸€æ¬¡ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹ç¼“å­˜
    store = get_store()
    
    # æ¸…é™¤ Streamlit ç¼“å­˜
    get_store.clear()
    
    # åˆ é™¤æ•´ä¸ªé›†åˆå¹¶é‡æ–°åˆ›å»ºï¼ˆæ›´å½»åº•ï¼‰
    store.delete_collection()
    st.info("ğŸ—‘ï¸ å·²æ¸…é™¤ä¸Šä¸€æ¬¡ä¸Šä¼ çš„æ‰€æœ‰æ–‡æ¡£")
    
    # é‡æ–°è·å– VectorStore å®ä¾‹
    store = get_store()
    
    tmpdir = os.path.join(".", "uploads")
    total_docs = 0
    total_time = time.time()
    
    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, uploaded_file in enumerate(uploaded_files):
        file_start_time = time.time()
        
        # æ›´æ–°è¿›åº¦
        progress = (idx + 1) / len(uploaded_files)
        progress_bar.progress(progress)
        status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        try:
            # ä¿å­˜æ–‡ä»¶
            path = save_uploaded_file(uploaded_file, tmpdir)
            
            # æå–æ–‡æœ¬
            extract_start = time.time()
            try:
                text = extract_text_from_file(path)
                # æ£€æŸ¥æå–çš„æ–‡æœ¬æ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­
                if not text or len(text.strip()) < 10:
                    st.warning(f"âš ï¸ æ–‡ä»¶ {uploaded_file.name} æå–çš„æ–‡æœ¬ä¸ºç©ºæˆ–å¤ªçŸ­ï¼ˆ{len(text)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®ç´¢å¼•")
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶ {uploaded_file.name} æ–‡æœ¬æå–å¤±è´¥: {e}")
                continue
            extract_time = time.time() - extract_start
            
            # åˆ†å—
            chunk_start = time.time()
            chunks = chunk_text(text)
            chunk_time = time.time() - chunk_start
            
            if not chunks:
                st.warning(f"âš ï¸ æ–‡ä»¶ {uploaded_file.name} æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
                continue
            
            # æ„å»ºæ–‡æ¡£
            docs = []
            for i, c in enumerate(chunks):
                docs.append({
                    "id": f"{uuid.uuid4()}",
                    "text": c,
                    "metadata": {
                        "source": uploaded_file.name,
                        "chunk_index": i,
                        "file_index": idx
                    },
                })
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            embed_start = time.time()
            store.add_documents(docs)
            embed_time = time.time() - embed_start
            
            total_docs += len(docs)
            file_time = time.time() - file_start_time
            
            # æ˜¾ç¤ºå•ä¸ªæ–‡ä»¶å¤„ç†ç»“æœ
            with st.expander(f"ğŸ“„ {uploaded_file.name} ({len(docs)} ä¸ªæ–‡æ¡£å—)"):
                st.write(f"- æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                st.write(f"- æ–‡æœ¬æå–: {extract_time:.2f}ç§’")
                st.write(f"- æ–‡æœ¬åˆ†å—: {chunk_time:.2f}ç§’")
                st.write(f"- Embedding ç”Ÿæˆ: {embed_time:.2f}ç§’")
                st.write(f"- æ–‡ä»¶å¤„ç†è€—æ—¶: {file_time:.2f}ç§’")
                # æ˜¾ç¤ºæ–‡æœ¬é¢„è§ˆ
                st.write(f"- æ–‡æœ¬é¢„è§ˆ: {text[:300]}...")
        
        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")
            import traceback
            with st.expander("é”™è¯¯è¯¦æƒ…"):
                st.code(traceback.format_exc())
            continue
    
    # å®Œæˆ
    progress_bar.progress(1.0)
    status_text.empty()
    total_time = time.time() - total_time
    
    st.success(f"âœ“ ç´¢å¼•å®Œæˆï¼å…±å¤„ç† {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {total_docs} ä¸ªæ–‡æ¡£å—")
    with st.expander("â±ï¸ æ€»ä½“æ€§èƒ½ç»Ÿè®¡"):
        st.write(f"- å¤„ç†æ–‡ä»¶æ•°: {len(uploaded_files)}")
        st.write(f"- æ€»æ–‡æ¡£å—æ•°: {total_docs}")
        st.write(f"- **æ€»è€—æ—¶: {total_time:.2f}ç§’**")
        st.write(f"- å¹³å‡æ¯ä¸ªæ–‡ä»¶: {total_time/len(uploaded_files):.2f}ç§’")


def main():
    st.title("RAG Chat â€” Streamlit + Chroma + OpenAI-Compat/Ollama")
    
    # æ˜¾ç¤º LLM æœåŠ¡çŠ¶æ€ï¼ˆåœ¨ä¾§è¾¹æ é¡¶éƒ¨ï¼‰
    llm_status = get_llm_status()
    with st.sidebar:
        st.header("LLM æœåŠ¡çŠ¶æ€")
        if llm_status["current_service"] == "OpenAI-compatible API":
            st.success(f"âœ… **å½“å‰ä½¿ç”¨: {llm_status['current_service']}**")
            st.write(f"æ¨¡å‹: {llm_status['current_model']}")
            if llm_status["fallback_service"] != "æ— ":
                st.info(f"å›é€€æœåŠ¡: {llm_status['fallback_service']}")
        elif llm_status["current_service"] == "Ollama":
            st.info(f"â„¹ï¸ **å½“å‰ä½¿ç”¨: {llm_status['current_service']}**")
            st.write(f"æ¨¡å‹: {llm_status['current_model']}")
        else:
            st.error(f"âŒ **{llm_status['current_service']}**")
            st.warning("è¯·é…ç½® openai-compatible API æˆ–ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ")
        
        # Display configuration details
        with st.expander("ğŸ“‹ é…ç½®è¯¦æƒ…"):
            st.write(f"**openai-compatible:** {'âœ… å·²é…ç½®' if llm_status['openai_configured'] else 'âŒ æœªé…ç½®'}")
            if not llm_status['openai_configured']:
                st.write("ç¼ºå°‘çš„é…ç½®:")
                if not llm_status.get('openai_api_key_set', False):
                    st.write("  - âŒ OPENAI_COMPATIBLE_API_KEY")
                st.write("")
                st.write("ğŸ’¡ **è§£å†³æ–¹æ³•:**")
                st.write("1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶")
                st.write("2. æ·»åŠ ä»¥ä¸‹é…ç½®:")
                st.code("""
OPENAI_COMPATIBLE_API_KEY=ä½ çš„APIå¯†é’¥
OPENAI_COMPATIBLE_MODEL=<model name>
                """, language="env")
            st.write(f"**Ollama:** {'âœ… å¯ç”¨' if llm_status['ollama_available'] else 'âŒ ä¸å¯ç”¨'}")
        
        st.divider()
    
    st.sidebar.header("Upload")
    st.sidebar.caption("æ”¯æŒä¸Šä¼ æœ€å¤š 5 ä¸ªæ–‡ä»¶ï¼ˆ.txt, .md, .pdf, .docxï¼‰")
    uploaded_files = st.sidebar.file_uploader(
        "Upload documents to index", 
        accept_multiple_files=True,
        type=['txt', 'md', 'pdf', 'docx']
    )
    
    if uploaded_files:
        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        if len(uploaded_files) > 5:
            st.sidebar.warning(f"âš ï¸ æ‚¨é€‰æ‹©äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œå°†åªå¤„ç†å‰ 5 ä¸ª")
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        with st.sidebar.expander(f"ğŸ“ å·²é€‰æ‹©æ–‡ä»¶ ({len(uploaded_files)} ä¸ª)"):
            for i, f in enumerate(uploaded_files[:5], 1):
                st.write(f"{i}. {f.name} ({f.size / 1024:.1f} KB)")
        
        if st.sidebar.button("Ingest", type="primary"):
            ingest_and_index(uploaded_files[:5])  # åªå¤„ç†å‰5ä¸ª

    st.header("Ask a question")
    question = st.text_input("Your question")
    
    # ä½¿ç”¨ session state è·Ÿè¸ªå½“å‰é—®é¢˜ï¼Œç¡®ä¿æ¯æ¬¡æ–°é—®é¢˜æ—¶éƒ½æ›´æ–°
    if 'last_question' not in st.session_state:
        st.session_state.last_question = ""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°é—®é¢˜
    is_new_question = question != st.session_state.last_question
    
    col1, col2 = st.columns(2)
    with col1:
        k = st.slider("retrieval k", 1, 10, 4, help="æ£€ç´¢çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡")
    with col2:
        min_similarity = st.slider(
            "æœ€å°ç›¸ä¼¼åº¦", 
            0.0, 1.0, 0.3, 0.05,
            help="ç›¸ä¼¼åº¦ä½äºæ­¤å€¼çš„ç‰‡æ®µå°†è¢«è¿‡æ»¤ï¼ˆ0.3 è¡¨ç¤º 30% ç›¸ä¼¼åº¦ï¼‰"
        )

    if st.button("Ask") and question:
        # æ›´æ–°é—®é¢˜è®°å½•
        st.session_state.last_question = question
        
        start_time = time.time()
        
        # Retrieval phase - æ¯æ¬¡éƒ½ä¼šé‡æ–°æ£€ç´¢
        retrieve_start = time.time()
        store = get_store()
        
        # å…ˆæ£€ç´¢åŸå§‹ç»“æœï¼ˆä¸åº”ç”¨é˜ˆå€¼ï¼‰ç”¨äºè°ƒè¯•
        raw_hits = store.query(question, k=k * 5, min_similarity=None)
        
        # ç„¶ååº”ç”¨é˜ˆå€¼è¿‡æ»¤
        hits = store.query(question, k=k, min_similarity=min_similarity)
        retrieve_time = time.time() - retrieve_start
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆæ¯æ¬¡æé—®éƒ½ä¼šæ›´æ–°ï¼‰
        with st.expander("ğŸ” æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆè°ƒè¯•ï¼‰", expanded=True):
            if not hits:
                st.warning("âš ï¸ æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³å†…å®¹ï¼")
                st.info(f"æç¤ºï¼šå°è¯•é™ä½æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå½“å‰: {min_similarity:.0%}ï¼‰")
                
                # æ˜¾ç¤ºæœªè¿‡æ»¤çš„åŸå§‹ç»“æœï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£è·ç¦»åˆ†å¸ƒ
                if raw_hits:
                    st.write("---")
                    st.write(f"**åŸå§‹æ£€ç´¢ç»“æœï¼ˆæœªè¿‡æ»¤ï¼Œå…± {len(raw_hits)} ä¸ªï¼‰ï¼š**")
                    for i, h in enumerate(raw_hits[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                        text = h.get('document') or h.get('text') or ""
                        metadata = h.get('metadata', {})
                        distance = h.get('distance', 0)
                        similarity = h.get('similarity', 0)
                        
                        st.write(f"**åŸå§‹ç‰‡æ®µ {i}:**")
                        st.write(f"- ç›¸ä¼¼åº¦: **{similarity:.2%}** (è·ç¦»: {distance:.4f})")
                        st.write(f"- æ¥æº: {metadata.get('source', 'æœªçŸ¥')}")
                        st.write(f"- å†…å®¹é¢„è§ˆ: {text[:150]}...")
                        st.write("---")
            else:
                threshold_info = f"ï¼ˆå·²è¿‡æ»¤ç›¸ä¼¼åº¦ < {min_similarity:.0%} çš„ç‰‡æ®µï¼‰"
                st.write(f"æ£€ç´¢åˆ° {len(hits)} ä¸ªç‰‡æ®µ{threshold_info}ï¼š")
                for i, h in enumerate(hits, 1):
                    st.write(f"**ç‰‡æ®µ {i}:**")
                    text = h.get('document') or h.get('text') or ""
                    metadata = h.get('metadata', {})
                    distance = h.get('distance', 0)
                    similarity = h.get('similarity', 0)
                    
                    # æ ¹æ®ç›¸ä¼¼åº¦æ˜¾ç¤ºä¸åŒçš„é¢œè‰²
                    if similarity >= 0.7:
                        similarity_color = "ğŸŸ¢"
                    elif similarity >= 0.5:
                        similarity_color = "ğŸŸ¡"
                    else:
                        similarity_color = "ğŸŸ "
                    
                    st.write(f"- {similarity_color} ç›¸ä¼¼åº¦: **{similarity:.2%}** (è·ç¦»: {distance:.4f})")
                    st.write(f"- æ¥æº: {metadata.get('source', 'æœªçŸ¥')}")
                    st.write(f"- å†…å®¹é•¿åº¦: {len(text)} å­—ç¬¦")
                    st.write(f"- å†…å®¹é¢„è§ˆ: {text[:200]}...")
                    st.write("---")
        
        prompt_start = time.time()
        prompt = build_prompt(question, hits)
        prompt_time = time.time() - prompt_start
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå®Œæ•´çš„ promptï¼ˆæ¯æ¬¡æé—®éƒ½ä¼šæ›´æ–°ï¼‰
        with st.expander("ğŸ“ å®Œæ•´ Promptï¼ˆè°ƒè¯•ï¼‰", expanded=False):
            st.code(prompt, language=None)
        
        st.info(f"ğŸ“Š Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦ | æ£€ç´¢è€—æ—¶: {retrieve_time:.2f}ç§’ | æ£€ç´¢åˆ° {len(hits)} ä¸ªç‰‡æ®µï¼ˆé˜ˆå€¼: {min_similarity:.0%}ï¼‰")
        
        # LLM generation phase
        with st.spinner("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
            generate_start = time.time()
            try:
                answer, service_used = generate(prompt)
                generate_time = time.time() - generate_start
                total_time = time.time() - start_time
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æœåŠ¡
                if service_used == "openai-compatible (Moonshot AI)":
                    st.success(f"âœ… ä½¿ç”¨æœåŠ¡: {service_used}")
                elif "Ollama" in service_used:
                    st.info(f"â„¹ï¸ ä½¿ç”¨æœåŠ¡: {service_used}")
                
                st.markdown("**Answer**")
                st.write(answer)
                
                st.markdown("**Sources**")
                for h in hits:
                    metadata = h.get("metadata", {})
                    similarity = h.get('similarity', 0)
                    st.write(f"- {metadata.get('source', 'æœªçŸ¥')} (å— {metadata.get('chunk_index', '?')}, ç›¸ä¼¼åº¦: {similarity:.2%})")
                
                # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                with st.expander("â±ï¸ æ€§èƒ½ç»Ÿè®¡"):
                    st.write(f"- å‘é‡æ£€ç´¢: {retrieve_time:.2f}ç§’")
                    st.write(f"- Prompt æ„å»º: {prompt_time:.2f}ç§’")
                    st.write(f"- **LLM ç”Ÿæˆ: {generate_time:.2f}ç§’** ({service_used})")
                    st.write(f"- **æ€»è€—æ—¶: {total_time:.2f}ç§’**")
                    
            except Exception as e:
                st.error(f"LLM generation error: {e}")


if __name__ == "__main__":
    main()
